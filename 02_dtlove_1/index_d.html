<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" href="../main.css" />
    <link rel="stylesheet" href="../style.css" />
    <link rel="icon" type="image/x-icon" href="../favicon.ico" />
    <title>Fang Yuan</title>
  </head>
  <body>
    <main>
      <!-- navigation -->
      <div class="header-bar container" id="myHeader">
        <!-- <div class="header-bar item left">
          <a href="/">Fang Yuan</a>
        </div>
        <div class="header-bar item right">
          <ul class="menu align-right">
            <li><a href="/">Works</a></li>
            <li><a href="/00_about/">About</a></li>
          </ul>
        </div> -->
      </div>

      <!-- works -->
      <div class="all-content container">
        <div class="grid-container">
          <div class="title grid">
            <p class="primary-title">DT LOVE</p>
            <p class="sub-title">
              A VR application for individuals with Autism Spectrum Disorder
              (ASD)
            </p>
            <hr />
            <ul class="short-intro">
              <li class="intro-type">
                Topic:
                <p class="intro-type-detail">VR, autism</p>
              </li>
              <li class="intro-type">
                Duration:
                <p class="intro-type-detail">
                  7 months [Nov 2019 - May 2020, Sep 2020 - Oct 2020]
                </p>
              </li>
              <li class="intro-type">
                Role:
                <p class="intro-type-detail">
                  Independent project, fullstack designer & developer
                </p>
              </li>
              <li class="intro-type">
                Prototyping Tools:
                <p class="intro-type-detail">
                  Unity 3d, SketchUp for Architecture, Photoshop, Illustrator,
                  Audacity
                </p>
              </li>
              <li class="intro-type">
                Programming language & SDK:
                <p class="intro-type-detail">C#, Oculus integration</p>
              </li>
              <li class="intro-type">
                Platform:
                <p class="intro-type-detail">Oculus Quest</p>
              </li>
            </ul>
          </div>
          <div class="long-intro grid">
            <p class="text red-left-border">
              Would Virtual Reality (VR) technology facilitate the learning
              process of individuals with Autism Spectrum Disorder (ASD)?
            </p>
            <p class="text no-border">
              DT LOVE is short for Discrete Trial Learning Over Virtual
              Environments. Originally it was a VR application aiming at
              “replicating” the training (proved to be effective for decades)
              for individuals with Autism spectrum disorder (ASD).
            </p>
            <div class="objective section">
              <p class="text green-left-border">OBJECTIVE</p>
              <ul class="section-list objective-list">
                <li>
                  <p class="text no-border">
                    Research Value. It would enable researchers to evaluate the
                    use of VR technology in such training ( the research was
                    proposed by Dr. Anibal Gutierrez from the Department of
                    Psychology, University of Miami).
                  </p>
                </li>
                <li>
                  <p class="text no-border">
                    Social & Business Value. Besides the research value, if
                    individuals with ASD could achieve similar or better
                    learning results in VR training like that in the live
                    training in the clinics or classrooms, it would allow them
                    to have more training with fewer costs, and the state of art
                    training would be available for more people.
                  </p>
                </li>
              </ul>
            </div>
          </div>
          <div class="main-content grid">
            <div class="trailer section">
              <p class="text green-left-border">TRAILER</p>
              <div class="video-container">
                <video poster="assets/v_00_Capstone trailer_new.png" controls>
                  <source
                    src="assets/v_00_Capstone trailer_new.mp4"
                    type="video/mp4"
                  />
                </video>
              </div>
            </div>
            <div class="process section">
              <p class="text green-left-border">PROCESS</p>
              <p class="text no-border">
                The process of the design and development for this VR
                application is quite different from projects in other platforms.
                There are some reasons resulting in the numerous iterations
                among Ideation & Design, Prototype & Implementation and Expert
                Review:
              </p>
              <ul class="section-list">
                <li>
                  <p class="text no-border">
                    no tools can really prototype a VR experience, except Unity
                    3D
                  </p>
                </li>
                <li>
                  <p class="text no-border">
                    lack of related works as precedents
                  </p>
                </li>
              </ul>
            </div>
            <div class="research section">
              <p class="text green-left-border">RESEARCH</p>
              <p class="text no-border">
                I visited the Intensive Behavioral Intervention Services (IBIS)
                clinic at the University of Miami at the beginning of the
                project. By immersing myself in the context and closely
                observing the various trainings for children with ASD, I got a
                vivid impression of how the training was done in reality. Then I
                met with the director of IBIS clinic, Dr. Anibal Gutierrez, to
                learn some background knowledge about ASD and the research he
                proposed.
              </p>
              <div class="docImage grid-container four">
                <img
                  class="docImage grid"
                  src="assets/img_04_01_Contextual Inquiry copy.png"
                  alt=""
                />
                <img
                  class="docImage grid"
                  src="assets/img_04_02_SME Interview copy.png"
                  alt=""
                />
                <img
                  class="docImage grid"
                  src="assets/img_04_03_Product Audit copy.png"
                  alt=""
                />
                <img
                  class="docImage grid"
                  src="assets/img_04_04_Technical Research copy.png"
                  alt=""
                />
              </div>
              <p class="text no-border">
                During the meeting, Dr. Gutierrez suggested me to simulate
                Discrete Trial Training (DTT). Discrete trial training (DTT) is
                a structured **Applied Behavior Analysis** (ABA) technique that
                was one of the first interventions developed for autism and had
                extensive research supporting it. A trial of DT training has
                three basic components: Antecedent, Behavior, and Consequence.
                Normally, Antecedent is the questions from instructors. Behavior
                is the response from students with ASD to the Antecedent.
                Consequence is the feedback of the behavior from instructors.
              </p>
              <p class="text no-border">
                Essentially, in this kind of training, the users’ task is to
                choose the right model as the instructor providing the verbal
                name of that object. Normally, the instructor would ask
                questions like “which one is …“or “touch the …“.
              </p>
              <p class="text no-border">
                The diagram beneath shows the basic structure of this type of
                training.
              </p>
              <div class="image narrow-container">
                <img
                  class="docImage"
                  src="assets/img_04_research-structure.png"
                  alt=""
                />
              </div>
            </div>
            <div class="interaction-design section">
              <p class="text green-left-border">INTERACTION DESIGN</p>
              <p class="text no-border">
                Prompts are the “cues” from the instructors that help students
                with ASD find the right answer. It happens between Antecedent
                and Behavior. In the real training context, when students
                hesitate or don’t respond to the Antecedent (question), normally
                instructors would provide prompts like repeating the question or
                holding one hand of the student and putting it on the target
                object.
              </p>
              <div class="image narrow-container">
                <img
                  class="docImage"
                  src="assets/img_05_interaction-design_00.png"
                  alt=""
                />
              </div>

              <div class="question one">
                <p class="text red-left-border">
                  How to translate different "prompts" to VR experience?
                </p>
                <p class="text orange-left-border">
                  1.1 Prompts in the live training
                </p>
                <img
                  class="docImage"
                  src="assets/img_05_interaction-design_01.png"
                  alt=""
                />
                <p class="text orange-left-border">
                  1.2 Prompts in the first iteration (storyboard)
                </p>
                <img
                  class="docImage"
                  src="assets/img_05_interaction-design_02.png"
                  alt=""
                />
                <p class="text orange-left-border">
                  1.3 Prompts in the final prototype
                </p>
                <div class="docImage grid-container four">
                  <img
                    class="docImage grid"
                    src="assets/img_05_interaction-design_prompt design_03-1.png"
                    alt=""
                  />
                  <img
                    class="docImage grid"
                    src="assets/img_05_interaction-design_prompt design_03-2.png"
                    alt=""
                  />
                  <img
                    class="docImage grid"
                    src="assets/img_05_interaction-design_prompt design_03-3.png"
                    alt=""
                  />
                  <img
                    class="docImage grid"
                    src="assets/img_05_interaction-design_prompt design_03-4.png"
                    alt=""
                  />
                </div>
                <p class="text orange-left-border">
                  1.4 The distribution of prompts along the timeline
                </p>
                <img
                  class="docImage"
                  src="assets/img_05_interaction-design_prompt design_04.png"
                  alt=""
                />
                <p class="text orange-left-border">
                  DEMO: Different prompts in a trial
                </p>
                <div class="video-container">
                  <video poster="assets/v_01_promts.png" controls>
                    <source src="assets/v_01_promts.mp4" type="video/mp4" />
                  </video>
                </div>
              </div>
              <div class="question two">
                <p class="text red-left-border">
                  2. How to enable users to select objects in VR?
                </p>
                <p class="text no-border">
                  The most important interaction of this VR system is selecting
                  the object in front of users. In the stage of Technical
                  Research, I tried three approaches shown as beneath. And
                  finally chose “using a virtual pointer with controller” due to
                  two reasons:
                </p>
                <ul class="section-list">
                  <li>
                    <p class="text no-border">
                      using controller would enable more accurate tracking of
                      users’ movements
                    </p>
                  </li>
                  <li>
                    <p class="text no-border">
                      using a virtual pointer would allow users to interact with
                      full-size objects at a certain distance
                    </p>
                  </li>
                </ul>
                <p class="text orange-left-border">
                  2.1 Technical research of "selecting" behaviors in VR
                </p>
                <div class="docImage grid-container three">
                  <img
                    class="docImage grid"
                    src="assets/img_05_interaction-design_selecting design_01.png"
                    alt=""
                  />
                  <img
                    class="docImage grid"
                    src="assets/img_05_interaction-design_selecting design_02.png"
                    alt=""
                  />
                  <img
                    class="docImage grid"
                    src="assets/img_05_interaction-design_selecting design_03.png"
                    alt=""
                  />
                </div>
                <p class="text orange-left-border">
                  2.2 Different "selecting" behaviors in the final prototype
                </p>
                <p class="text no-border">
                  In the scope of the detailed VR experience, selecting
                  behaviors are separated by three types of interactive objects,
                  allowing users to be more aware of the difference between
                  these objects. To differentiate different selecting behaviors,
                  I mainly used different auditory feedbacks for hovering and
                  triggering (behaviors of virtual pointer) to do the
                  separation.
                </p>
                <div class="docImage grid-container three">
                  <img
                    class="docImage grid"
                    src="assets/img_05_interaction-design_selecting design_04.png"
                    alt=""
                  />
                  <img
                    class="docImage grid"
                    src="assets/img_05_interaction-design_selecting design_05.png"
                    alt=""
                  />
                  <img
                    class="docImage grid"
                    src="assets/img_05_interaction-design_selecting design_06.png"
                    alt=""
                  />
                </div>
                <p class="text orange-left-border">
                  DEMO: Select different objects
                </p>
                <div class="video-container">
                  <video poster="assets/v_02_selectItem.png" controls>
                    <source src="assets/v_02_selectItem.mp4" type="video/mp4" />
                  </video>
                </div>
              </div>
              <div class="question three">
                <p class="text red-left-border">
                  3. How to translate different "rewards" to VR experience?
                </p>
                <p class="text no-border">
                  The “feedback” here refers to instructors’ response after
                  students with ASD choose the answer. If the answer were wrong,
                  the instructor would simply tell the student it isn’t the
                  right answer and clean the field, then prepare for the next
                  trial. If the answer were correct, instructors would reward
                  them with verbal praise or toys, like a toy car, bubbles, a
                  short clip of cartoon, or physical interactions like giving a
                  high five or a swing, tickling, hugging.
                </p>
                <p class="text orange-left-border">
                  3.1 Rewards in the live training
                </p>
                <div class="docImage grid-container three">
                  <img
                    class="docImage grid"
                    src="assets/img_05_interaction-design_reward design_01.png"
                  />
                  <img
                    class="docImage grid"
                    src="assets/img_05_interaction-design_reward design_02.png"
                  />
                  <img
                    class="docImage grid"
                    src="assets/img_05_interaction-design_reward design_03.png"
                  />
                  <img
                    class="docImage grid"
                    src="assets/img_05_interaction-design_reward design_04.png"
                  />
                  <img
                    class="docImage grid"
                    src="assets/img_05_interaction-design_reward design_05.png"
                  />
                  <img
                    class="docImage grid"
                    src="assets/img_05_interaction-design_reward design_06.png"
                  />
                </div>
                <div class="container two-column">
                  <div class="column">
                    <p class="text orange-left-border">
                      3.2 Rewards in the first iteration (storyboard)
                    </p>
                    <img
                      class="docImage"
                      src="assets/img_05_interaction-design_reward design_07.png"
                      alt=""
                    />
                  </div>
                  <div class="column">
                    <p class="text orange-left-border">
                      3.3 Rewards in the final prototype
                    </p>
                    <img
                      class="docImage"
                      src="assets/img_05_interaction-design_reward design_08.png"
                      alt=""
                    />
                  </div>
                </div>

                <p class="text orange-left-border">
                  DEMO: The Reward for the Right Answer
                </p>
                <div class="video-container">
                  <video poster="assets/v_03_reward rightAnswer.png" controls>
                    <source
                      src="assets/v_03_reward rightAnswer.mp4"
                      type="video/mp4"
                    />
                  </video>
                </div>
                <p class="text orange-left-border">DEMO: a complete session</p>
                <div class="video-container">
                  <video poster="assets/v_04_a session.png" controls>
                    <source src="assets/v_04_a session.mp4" type="video/mp4" />
                  </video>
                </div>
              </div>
            </div>
            <div class="3d-interface-design section">
              <p class="text orange-left-border">Spatial Layout</p>
              <img
                class="docImage"
                src="assets/img_06_3d interface design_spatial layout.png"
              />
              <p class="text orange-left-border">Field Of View (FOV)</p>
              <p class="text no-border">
                The field of view (FOV) is the angular measure of what can be
                seen at a single point in time. Normally, the FOV of a VR
                headset like Oculus Quest would be less than that of a person
                with naked eyes. In this project, users need to stand or sit
                stationarily to choose the correct answer from the models in
                front of them. Therefore, besides other interactive objects like
                buttons on the navigation panel, arrange all the models within
                the horizontal and vertical FOV of the VR headset is crucial for
                the performance of the users and the result of the research it
                assisting.
              </p>
              <p class="text no-border">
                The horizontal and vertical FOV of Oculus Quest varies in
                different reports, blogs, or posts online. Therefore, I decided
                to measure them by meself using a matrix of cubes in Unity. With
                an Interpupillary Distance (IPD) of 62, the user’s horizontal
                FOV is 102°, and the vertical FOV is 100°.
              </p>
              <img
                class="docImage"
                src="assets/img_06_3d interface design_spatial layout_fov.png"
              />
            </div>
            <div class="implementation section">
              <p class="text no-border">
                For this project, the implementation was a part of “ideation &
                Design.” Only by implementing features in Unity 3d with all the
                logic of training and effects can the design and development
                move forwards.
              </p>
              <p class="text no-border">
                The detailed process of implementation is well recorded in my
                design journal.
              </p>
            </div>
            <div class="testing-evaluation section">
              <p class="text orange-left-border">Preparation</p>
              <p class="text no-border">
                In fall 2020, Dr. Anibal Gutierrez from the Department of
                Psychology, University of Miami, proposed a new study in the
                context of Covid-19. The study’s goal is to evaluate the VR
                system with non-ASD subjects to refine the procedures first,
                which would act as the preparation for the evaluation with
                individuals with ASD afterward.
              </p>
              <p class="text no-border">
                To realize the new study’s goal, Dr. Gutierrez provided the new
                training structure and suggested replacing the current animal
                models with something non-ASD subjects didn’t already know. For
                example, they can be Japanese or Chinese characters.
              </p>
              <p class="text blue-left-border">
                The task structure is shown as beneath:
              </p>
              <img
                class="docImage"
                src="assets/img_08_testing & evaluation_new structure.png"
                alt=""
              />
              <p class="text orange-left-border">Final Testing</p>
              <p class="text no-border">
                The final testing would be conducted in the Department of
                Psychology, University of Miami, in Spring 2021.
              </p>
              <p class="text orange-left-border">DEMO: New study</p>
              <div class="video-container">
                <video poster="assets/v_05_new study.png" controls>
                  <source src="assets/v_05_new study.mp4" type="video/mp4" />
                </video>
              </div>
            </div>
            <div class="reflection section">
              <p class="text no-border">
                This project gives me a great opportunity to create a VR
                application from scratch: from research to ideation, to design,
                to prototype, to implementation, and justify design decisions
                through numerous iterations with professionals in both fields of
                design and psychology.
              </p>
              <p class="text no-border">
                In this project, I explored a lot of 3d effects I learned from
                other media, like movies and anime. For me, as a designer for 3d
                real context before, the amazing effects a VR system can present
                are mind-blowing. And the professionals in design and psychology
                were all very optimistic and exciting about the project.
              </p>
              <p class="text orange-left-border">Limitation & Concerns</p>
              <ul class="section-list">
                <li>
                  <p class="text no-border">
                    Customization: currently, the project can’t enable in-app
                    customization. Even though I’d arranged the settings in
                    Unity 3d to allow a person to do the customization almost
                    without any experience of Unity 3d, it would be an obstacle
                    for the future research without a unity developer.
                  </p>
                </li>
                <li>
                  <p class="text no-border">
                    Auto recording system: one advantage of a Virtual Reality
                    system is that it can record various data from users during
                    use. This feature would significantly reduce the workload of
                    instructors in clinics or classrooms. Currently, the
                    application can only record the scores when users pass a
                    level.
                  </p>
                </li>
                <li>
                  <p class="text no-border">
                    Target Users: Oculus Quest, together with other VR headsets,
                    all have a similar age limit for users. For Oculus Quest, it
                    is 13+. Even though there might be VR headsets designed for
                    younger children in the future, at this point, the target
                    users chosen for the testing for this project should at
                    least be 13 years old.
                  </p>
                </li>
                <li>
                  <p class="text no-border">
                    Lack of design guidelines: when I started the project’s
                    design process, I couldn’t find any design guidelines about
                    creating VR applications for individuals with ASD. All the
                    design decisions I made were based on training in the real
                    context, my own design experience, and suggestions from
                    psychologists and designers. Before the final evaluation of
                    the application, it remains unknown whether these designs
                    are good practice for the target users.
                  </p>
                </li>
              </ul>
              <p class="text orange-left-border">Next Step</p>
              <ul class="section-list">
                <li>
                  <p class="text no-border">
                    Study the results of the testing in the Department of
                    Psychology.
                  </p>
                </li>
                <li>
                  <p class="text no-border">
                    Design and conduct testing from HCI researchers’ perspective
                    to extract design insights about designing the VR system for
                    individuals with ASD.
                  </p>
                </li>
                <li>
                  <p class="text no-border">
                    Explore other increasing mature VR features, like hand
                    tracking, voice command, eye tracking, etc.
                  </p>
                </li>
                <li>
                  <p class="text no-border">
                    Explore the use of VR for different types of training for
                    individuals with ASD, like training for communication
                    skills.
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>

      <!-- copyright -->
      <div class="copyright container" id="myCopyright">
        <!-- <div class="copyright item">
          <a href="/">© Fang Yuan</a>
        </div> -->
      </div>
    </main>
    <script src="../index.js"></script>
  </body>
</html>
